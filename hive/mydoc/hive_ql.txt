
问题导读：
1.什么情况下，可以不启用MapReduce Job？
2.方法1通过什么方式，不启用job?
3.bin/hive --hiveconf hive.fetch.task.conversion=more的作用是什么？
4.如果一直开启不使用MapReduce Job，该如何配置？







如果你想查询某个表的某一列，Hive默认是会启用MapReduce Job来完成这个任务，如下：
hive> SELECT id, money FROM m limit 10;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Cannot run job locally: Input Size (= 235105473) is larger than
hive.exec.mode.local.auto.inputbytes.max (= 134217728)
Starting Job = job_1384246387966_0229, Tracking URL =

http://l-datalogm1.data.cn1:9981/proxy/application_1384246387966_0229/

Kill Command = /home/q/hadoop-2.2.0/bin/hadoop job
-kill job_1384246387966_0229
hadoop job information for Stage-1: number of mappers: 1;
number of reducers: 0
2013-11-13 11:35:16,167 Stage-1 map = 0%,  reduce = 0%
2013-11-13 11:35:21,327 Stage-1 map = 100%,  reduce = 0%,
Cumulative CPU 1.26 sec
2013-11-13 11:35:22,377 Stage-1 map = 100%,  reduce = 0%,
Cumulative CPU 1.26 sec
MapReduce Total cumulative CPU time: 1 seconds 260 msec
Ended Job = job_1384246387966_0229
MapReduce Jobs Launched:
Job 0: Map: 1   Cumulative CPU: 1.26 sec
HDFS Read: 8388865 HDFS Write: 60 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 260 msec
OK
1       122
1       185
1       231
1       292
1       316
1       329
1       355
1       356
1       362
1       364
Time taken: 16.802 seconds, Fetched: 10 row(s)
复制代码

　我们都知道，启用MapReduce Job是会消耗系统开销的。对于这个问题，从Hive0.10.0版本开始，对于简单的不需要聚合的类似SELECT <col> from <table> LIMIT n语句，不需要起MapReduce job，直接通过Fetch task获取数据，可以通过下面几种方法实现：

方法一：
hive> set hive.fetch.task.conversion=more;
hive> SELECT id, money FROM m limit 10;
OK
1       122
1       185
1       231
1       292
1       316
1       329
1       355
1       356
1       362
1       364
Time taken: 0.138 seconds, Fetched: 10 row(s)
复制代码

上面 set hive.fetch.task.conversion=more;开启了Fetch任务，所以对于上述简单的列查询不在启用MapReduce job！

方法二：

bin/hive --hiveconf hive.fetch.task.conversion=more
复制代码

方法三：
上面的两种方法都可以开启了Fetch任务，但是都是临时起作用的；如果你想一直启用这个功能，可以在${HIVE_HOME}/conf/hive-site.xml里面加入以下配置：

<property>
  <name>hive.fetch.task.conversion</name>
  <value>more</value>
  <description>
    Some select queries can be converted to single FETCH task
    minimizing latency.Currently the query should be single
    sourced not having any subquery and should not have
    any aggregations or distincts (which incurrs RS),
    lateral views and joins.
    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)
  </description>
</property>
复制代码
这样就可以长期启用Fetch任务了，很不错吧，也赶紧去试试吧！


问题导读：

1.hive通过什么命令可以执行文件中sql语句
2.　source /home/wyp/Documents/test;与hive -f test的区别是什么？





Hive可以运行保存在文件里面的一条或多条的语句，只要用-f参数，一般情况下，保存这些Hive查询语句的文件通常用.q或者.hql后缀名，但是这不是必须的，你也可以保存你想要的后缀名。假设test文件里面有一下的Hive查询语句：
select * from p limit 10;
select count(*) from p;
复制代码

那么我们可以用下面的命令来查询：
[wyp@wyp hive-0.11.0-bin]$ bin/hive -f test

........这里省略了一些输出...........

OK
196        242        3        881250949        20131102        jx
186        302        3        891717742        20131102        jx
22        377        1        878887116        20131102        jx
244        51        2        880606923        20131102        jx
166        346        1        886397596        20131102        jx
298        474        4        884182806        20131102        jx
115        265        2        881171488        20131102        jx
253        465        5        891628467        20131102        jx
305        451        3        886324817        20131102        jx
6        86        3        883603013        20131102        jx
Time taken: 4.386 seconds, Fetched: 10 row(s)

........这里省略了一些输出...........

OK
4000000
Time taken: 16.284 seconds, Fetched: 1 row(s)
复制代码

如果你配置好了Hive shell的路径，你可以用SOURCE命令来运行那个查询文件:
[wyp@wyp hive-0.11.0-bin]$ hive
hive> source /home/wyp/Documents/test;

........这里省略了一些输出...........

OK
196        242        3        881250949        20131102        jx
186        302        3        891717742        20131102        jx
22        377        1        878887116        20131102        jx
244        51        2        880606923        20131102        jx
166        346        1        886397596        20131102        jx
298        474        4        884182806        20131102        jx
115        265        2        881171488        20131102        jx
253        465        5        891628467        20131102        jx
305        451        3        886324817        20131102        jx
6        86        3        883603013        20131102        jx
Time taken: 4.386 seconds, Fetched: 10 row(s)

........这里省略了一些输出...........

OK
4000000
Time taken: 16.284 seconds, Fetched: 1 row(s)


问题导读
1.从本地文件系统中通过什么命令可导入数据到Hive表？
2.什么是动态分区插入？
3.该如何实现动态分区插入？

扩展：
这里可以和Hive中的三种不同的数据导出方式介绍进行对比？






Hive的几种常见的数据导入方式
这里介绍四种：
（1）、从本地文件系统中导入数据到Hive表；
（2）、从HDFS上导入数据到Hive表；
（3）、从别的表中查询出相应的数据并导入到Hive表中；
（4）、在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中。


一、从本地文件系统中导入数据到Hive表
先在Hive里面创建好表，如下：
hive> create table wyp
    > (id int, name string,
    > age int, tel string)
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY '\t'
    > STORED AS TEXTFILE;
OK
Time taken: 2.832 seconds
复制代码

这个表很简单，只有四个字段，具体含义我就不解释了。本地文件系统里面有个/home/wyp/wyp.txt文件，内容如下：
[wyp@master ~]$ cat wyp.txt
1       wyp     25      13188888888888
2       test    30      13888888888888
3       zs      34      899314121
复制代码

wyp.txt文件中的数据列之间是使用\t分割的，可以通过下面的语句将这个文件里面的数据导入到wyp表里面，操作如下：
hive> load data local inpath 'wyp.txt' into table wyp;
Copying data from file:/home/wyp/wyp.txt
Copying file: file:/home/wyp/wyp.txt
Loading data to table default.wyp
Table default.wyp stats:
[num_partitions: 0, num_files: 1, num_rows: 0, total_size: 67]
OK
Time taken: 5.967 seconds
复制代码

这样就将wyp.txt里面的内容导入到wyp表里面去了，可以到wyp表的数据目录下查看，如下命令：

hive> dfs -ls /user/hive/warehouse/wyp ;
Found 1 items
-rw-r--r--3 wyp supergroup 67 2014-02-19 18:23 /hive/warehouse/wyp/wyp.txt
复制代码

需要注意的是：

和我们熟悉的关系型数据库不一样，Hive现在还不支持在insert语句里面直接给出一组记录的文字形式，也就是说，Hive并不支持INSERT INTO …. VALUES形式的语句。

二、HDFS上导入数据到Hive表
　　从本地文件系统中将数据导入到Hive表的过程中，其实是先将数据临时复制到HDFS的一个目录下（典型的情况是复制到上传用户的HDFS home目录下,比如/home/wyp/），然后再将数据从那个临时目录下移动（注意，这里说的是移动，不是复制！）到对应的Hive表的数据目录里面。既然如此，那么Hive肯定支持将数据直接从HDFS上的一个目录移动到相应Hive表的数据目录下，假设有下面这个文件/home/wyp/add.txt，具体的操作如下：
[wyp@master /home/q/hadoop-2.2.0]$ bin/hadoop fs -cat /home/wyp/add.txt
5       wyp1    23      131212121212
6       wyp2    24      134535353535
7       wyp3    25      132453535353
8       wyp4    26      154243434355
复制代码

上面是需要插入数据的内容，这个文件是存放在HDFS上/home/wyp目录（和一中提到的不同，一中提到的文件是存放在本地文件系统上）里面，我们可以通过下面的命令将这个文件里面的内容导入到Hive表中，具体操作如下：

hive> load data inpath '/home/wyp/add.txt' into table wyp;
Loading data to table default.wyp
Table default.wyp stats:
[num_partitions: 0, num_files: 2, num_rows: 0, total_size: 215]
OK
Time taken: 0.47 seconds

hive> select * from wyp;
OK
5       wyp1    23      131212121212
6       wyp2    24      134535353535
7       wyp3    25      132453535353
8       wyp4    26      154243434355
1       wyp     25      13188888888888
2       test    30      13888888888888
3       zs      34      899314121
Time taken: 0.096 seconds, Fetched: 7 row(s)
复制代码

从上面的执行结果我们可以看到，数据的确导入到wyp表中了！请注意load data inpath ‘/home/wyp/add.txt’ into table wyp;里面是没有local这个单词的，这个是和一中的区别。

三、从别的表中查询出相应的数据并导入到Hive表中
假设Hive中有test表，其建表语句如下所示：

hive> create table test(
    > id int, name string
    > ,tel string)
    > partitioned by
    > (age int)
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY '\t'
    > STORED AS TEXTFILE;
OK
Time taken: 0.261 seconds


大体和wyp表的建表语句类似，只不过test表里面用age作为了分区字段。对于分区，这里在做解释一下：
分区：在Hive中，表的每一个分区对应表下的相应目录，所有分区的数据都是存储在对应的目录中。比如wyp表有dt和city两个分区，则对应dt=20131218,city=BJ对应表的目录为/user/hive/warehouse/dt=20131218/city=BJ，所有属于这个分区的数据都存放在这个目录中。

下面语句就是将wyp表中的查询结果并插入到test表中：
hive> insert into table test
    > partition (age='25')
    > select id, name, tel
    > from wyp;
#####################################################################
           这里输出了一堆Mapreduce任务信息，这里省略
#####################################################################
Total MapReduce CPU Time Spent: 1 seconds 310 msec
OK
Time taken: 19.125 seconds

hive> select * from test;
OK
5       wyp1    131212121212    25
6       wyp2    134535353535    25
7       wyp3    132453535353    25
8       wyp4    154243434355    25
1       wyp     13188888888888  25
2       test    13888888888888  25
3       zs      899314121       25
Time taken: 0.126 seconds, Fetched: 7 row(s)

这里做一下说明：
我们知道我们传统数据块的形式insert into table values（字段1，字段2），这种形式hive是不支持的。

通过上面的输出，我们可以看到从wyp表中查询出来的东西已经成功插入到test表中去了！如果目标表（test）中不存在分区字段，可以去掉partition (age=’25′)语句。当然，我们也可以在select语句里面通过使用分区值来动态指明分区：
hive> set hive.exec.dynamic.partition.mode=nonstrict;
hive> insert into table test
    > partition (age)
    > select id, name,
    > tel, age
    > from wyp;
#####################################################################
           这里输出了一堆Mapreduce任务信息，这里省略
#####################################################################
Total MapReduce CPU Time Spent: 1 seconds 510 msec
OK
Time taken: 17.712 seconds


hive> select * from test;
OK
5       wyp1    131212121212    23
6       wyp2    134535353535    24
7       wyp3    132453535353    25
1       wyp     13188888888888  25
8       wyp4    154243434355    26
2       test    13888888888888  30
3       zs      899314121       34
Time taken: 0.399 seconds, Fetched: 7 row(s)


这种方法叫做动态分区插入，但是Hive中默认是关闭的，所以在使用前需要先把hive.exec.dynamic.partition.mode设置为nonstrict。当然，Hive也支持insert overwrite方式来插入数据，从字面我们就可以看出，overwrite是覆盖的意思，是的，执行完这条语句的时候，相应数据目录下的数据将会被覆盖！而insert into则不会，注意两者之间的区别。例子如下：

hive> insert overwrite table test
    > PARTITION (age)
    > select id, name, tel, age
    > from wyp;


更可喜的是，Hive还支持多表插入，什么意思呢？在Hive中，我们可以把insert语句倒过来，把from放在最前面，它的执行效果和放在后面是一样的，如下：
hive> show create table test3;
OK
CREATE  TABLE test3(
  id int,
  name string)
Time taken: 0.277 seconds, Fetched: 18 row(s)

hive> from wyp
    > insert into table test
    > partition(age)
    > select id, name, tel, age
    > insert into table test3
    > select id, name
    > where age>25;

hive> select * from test3;
OK
8       wyp4
2       test
3       zs
Time taken: 4.308 seconds, Fetched: 3 row(s)
复制代码

可以在同一个查询中使用多个insert子句，这样的好处是我们只需要扫描一遍源表就可以生成多个不相交的输出。这个很酷吧！

四、在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中
在实际情况中，表的输出结果可能太多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接存在一个新的表中是非常方便的，我们称这种情况为CTAS（create table .. as select）如下：

hive> create table test4
    > as
    > select id, name, tel
    > from wyp;

hive> select * from test4;
OK
5       wyp1    131212121212
6       wyp2    134535353535
7       wyp3    132453535353
8       wyp4    154243434355
1       wyp     13188888888888
2       test    13888888888888
3       zs      899314121
Time taken: 0.089 seconds, Fetched: 7 row(s)
复制代码

数据就插入到test4表中去了，CTAS操作是原子的，因此如果select查询由于某种原因而失败，新表是不会创建的！


本帖最后由 hyj 于 2014-4-19 00:51 编辑

问题导读：
1.导出本地文件系统和hdfs文件系统区别是什么？
2.带有local命令是指导出本地还是hdfs文件系统？
3.hive中，使用的insert与传统数据库insert的区别是什么？
4.导出数据如何自定义分隔符？





今天我们再谈谈Hive中的三种不同的数据导出方式。
根据导出的地方不一样，将这些方式分为三种：
（1）、导出到本地文件系统；
（2）、导出到HDFS中；
（3）、导出到Hive的另一个表中。
为了避免单纯的文字，我将一步一步地用命令进行说明。


一、导出到本地文件系统

　　
hive> insert overwrite local directory '/home/wyp/wyp'
    > select * from wyp;
复制代码

这条HQL的执行需要启用Mapreduce完成，运行完这条语句之后，将会在本地文件系统的/home/wyp/wyp目录下生成文件，这个文件是Reduce产生的结果（这里生成的文件名是000000_0），我们可以看看这个文件的内容：

[wyp@master ~/wyp]$ vim 000000_0
5^Awyp1^A23^A131212121212
6^Awyp2^A24^A134535353535
7^Awyp3^A25^A132453535353
8^Awyp4^A26^A154243434355
1^Awyp^A25^A13188888888888
2^Atest^A30^A13888888888888
3^Azs^A34^A899314121
复制代码

可以看出，这就是wyp表中的所有数据。数据中的列与列之间的分隔符是^A(ascii码是\00001)。

和导入数据到Hive不一样，不能用insert into来将数据导出：

　　
hive> insert into local directory '/home/wyp/wyp'
    > select * from wyp;
NoViableAltException(79@[])
        at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:683)
        at org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:30667)
        at org.apache.hadoop.hive.ql.parse.HiveParser.regular_body(HiveParser.java:28421)
        at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatement(HiveParser.java:28306)
        at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:28100)
        at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1213)
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:928)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:190)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
FAILED: ParseException line 1:12 missing TABLE at 'local' near 'local' in select clause
line 1:18 cannot recognize input near 'directory' ''/home/wyp/wyp'' 'select' in select clause
复制代码

二、导出到HDFS中
和导入数据到本地文件系统一样的简单，可以用下面的语句实现：

　　
hive> insert overwrite directory '/home/wyp/hdfs'
    > select * from wyp;
复制代码
将会在HDFS的/home/wyp/hdfs目录下保存导出来的数据。注意，和导出文件到本地文件系统的HQL少一个local，数据的存放路径就不一样了。

三、导出到Hive的另一个表中

这也是Hive的数据导入方式，如下操作：

　
hive> insert into table test
    > partition (age='25')
    > select id, name, tel
    > from wyp;
#####################################################################
           这里输出了一堆Mapreduce任务信息，这里省略
#####################################################################
Total MapReduce CPU Time Spent: 1 seconds 310 msec
OK
Time taken: 19.125 seconds

hive> select * from test;
OK
5       wyp1    131212121212    25
6       wyp2    134535353535    25
7       wyp3    132453535353    25
8       wyp4    154243434355    25
1       wyp     13188888888888  25
2       test    13888888888888  25
3       zs      899314121       25
Time taken: 0.126 seconds, Fetched: 7 row(s)
复制代码

细心的读者可能会问，怎么导入数据到文件中，数据的列之间为什么不是wyp表设定的列分隔符呢？其实在Hive 0.11.0版本之间，数据的导出是不能指定列之间的分隔符的，只能用默认的列分隔符，也就是上面的^A来分割，这样导出来的数据很不直观，看起来很不方便！
如果你用的Hive版本是0.11.0，那么你可以在导出数据的时候来指定列之间的分隔符。

下面详细介绍：


在Hive0.11.0版本新引进了一个新的特性，也就是当用户将Hive查询结果输出到文件，用户可以指定列的分割符，而在之前的版本是不能指定列之间的分隔符，这样给我们带来了很大的不变，在Hive0.11.0之前版本我们一般是这样用的：
hive> insert overwrite local directory '/home/wyp/Documents/result'
hive> select * from test;
复制代码
保存的文件列之间是用^A（\x01）来分割
196^A242^A3
186^A302^A3
22^A377^A1
244^A51^A2
复制代码
注意，上面是为了显示方便，而将\x01写作^A，在实际的文本编辑器我们是看不到^A的，而是一个奇怪的符号。现在我们可以用Hive0.11.0版本新引进了一个新的特性，指定输出结果列之间的分隔符：
hive> insert overwrite local directory '/home/wyp/Documents/result'
hive> row format delimited
hive> fields terminated by '\t'
hive> select * from test;
复制代码
再次看出输出的结果
196        242        3
186        302        3
22        377        1
244        51        2
复制代码

结果好看多了。如果是map类型可以用下面语句来分割map的key和value
hive> insert overwrite local directory './test-04'
hive> row format delimited
hive> FIELDS TERMINATED BY '\t'
hive> COLLECTION ITEMS TERMINATED BY ','
hive> MAP KEYS TERMINATED BY ':'
hive> select * from src;
复制代码



根据上面内容，我们来进一步操作：


hive> insert overwrite local directory '/home/yangping.wu/local'
    > row format delimited
    > fields terminated by '\t'
    > select * from wyp;
复制代码
[wyp@master ~/local]$ vim 000000_0
5       wyp1    23      131212121212
6       wyp2    24      134535353535
7       wyp3    25      132453535353
8       wyp4    26      154243434355
1       wyp     25      13188888888888
2       test    30      13888888888888
3       zs      34      899314121
复制代码

其实，我们还可以用hive的-e和-f参数来导出数据。其中-e 表示后面直接接带双引号的sql语句；而-f是接一个文件，文件的内容为一个sql语句，如下：

　　
[wyp@master ~/local][        DISCUZ_CODE_26        ]nbsp; hive -e "select * from wyp" >> local/wyp.txt
[wyp@master ~/local][        DISCUZ_CODE_26        ]nbsp; cat wyp.txt
5       wyp1    23      131212121212
6       wyp2    24      134535353535
7       wyp3    25      132453535353
8       wyp4    26      154243434355
1       wyp     25      13188888888888
2       test    30      13888888888888
3       zs      34      899314121
复制代码
得到的结果也是用\t分割的。也可以用-f参数实现：

[wyp@master ~/local]$ cat wyp.sql
select * from wyp
[wyp@master ~/local]$ hive -f wyp.sql >> local/wyp2.txt
复制代码

上述语句得到的结果也是\t分割的。




问题导读：
1.如何对表创建索引？
2.与创建表有什么区别？






索引是标准的数据库技术，hive 0.7版本之后支持索引。Hive提供有限的索引功能，这不像传统的关系型数据库那样有“键(key)”的概念，用户可以在某些列上创建索引来加速某些操作，给一个表创建的索引数据被保存在另外的表中。 Hive的索引功能现在还相对较晚，提供的选项还较少。但是，索引被设计为可使用内置的可插拔的java代码来定制，用户可以扩展这个功能来满足自己的需求。 当然不是说有的查询都会受惠于Hive索引。用户可以使用EXPLAIN语法来分析HiveQL语句是否可以使用索引来提升用户查询的性能。像RDBMS中的索引一样，需要评估索引创建的是否合理，毕竟，索引需要更多的磁盘空间，并且创建维护索引也会有一定的代价。 用户必须要权衡从索引得到的好处和代价。

下面说说怎么创建索引：
1、先创建表：
hive> create table user( id int, name string)
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY '\t'
    > STORED AS TEXTFILE;


2、导入数据：
hive> load data local inpath '/export1/tmp/wyp/row.txt'
    > overwrite into table user;


3、创建索引之前测试
hive> select * from user where id =500000;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Cannot run job locally: Input Size (= 356888890) is larger than
hive.exec.mode.local.auto.inputbytes.max (= 134217728)
Starting Job = job_1384246387966_0247, Tracking URL =

http://l-datalogm1.data.cn1:9981/proxy/application_1384246387966_0247/

Kill Command=/home/q/hadoop/bin/hadoop job -kill job_1384246387966_0247
Hadoop job information for Stage-1: number of mappers:2; number of reducers:0
2013-11-13 15:09:53,336 Stage-1 map = 0%,  reduce = 0%
2013-11-13 15:09:59,500 Stage-1 map=50%,reduce=0%, Cumulative CPU 2.0 sec
2013-11-13 15:10:00,531 Stage-1 map=100%,reduce=0%, Cumulative CPU 5.63 sec
2013-11-13 15:10:01,560 Stage-1 map=100%,reduce=0%, Cumulative CPU 5.63 sec
MapReduce Total cumulative CPU time: 5 seconds 630 msec
Ended Job = job_1384246387966_0247
MapReduce Jobs Launched:
Job 0: Map: 2   Cumulative CPU: 5.63 sec
HDFS Read: 361084006 HDFS Write: 357 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 630 msec
OK
500000 wyp.
Time taken: 14.107 seconds, Fetched: 1 row(s)

一共用了14.107s

4、对user创建索引
hive> create index user_index on table user(id)
    > as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
    > with deferred rebuild
    > IN TABLE user_index;
hive> alter index user_index on user rebuild;
hive> select * from user_index_table limit 5;
0       hdfs://mycluster/user/hive/warehouse/table02/000000_0   [0]
1       hdfs://mycluster/user/hive/warehouse/table02/000000_0   [352]
2       hdfs://mycluster/user/hive/warehouse/table02/000000_0   [704]
3       hdfs://mycluster/user/hive/warehouse/table02/000000_0   [1056]
4       hdfs://mycluster/user/hive/warehouse/table02/000000_0   [1408]
Time taken: 0.244 seconds, Fetched: 5 row(s)
复制代码



这样就对user表创建好了一个索引。

5、对创建索引后的user再进行测试
hive> select * from user where id =500000;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Cannot run job locally: Input Size (= 356888890) is larger than
hive.exec.mode.local.auto.inputbytes.max (= 134217728)
Starting Job = job_1384246387966_0247, Tracking URL =

http://l-datalogm1.data.cn1:9981/proxy/application_1384246387966_0247/

Kill Command=/home/q/hadoop/bin/hadoop job -kill job_1384246387966_0247
Hadoop job information for Stage-1: number of mappers:2; number of reducers:0
2013-11-13 15:23:12,336 Stage-1 map = 0%,  reduce = 0%
2013-11-13 15:23:53,240 Stage-1 map=50%,reduce=0%, Cumulative CPU 2.0 sec
2013-11-13 15:24:00,253 Stage-1 map=100%,reduce=0%, Cumulative CPU 5.27 sec
2013-11-13 15:24:01,650 Stage-1 map=100%,reduce=0%, Cumulative CPU 5.27 sec
MapReduce Total cumulative CPU time: 5 seconds 630 msec
Ended Job = job_1384246387966_0247
MapReduce Jobs Launched:
Job 0: Map: 2   Cumulative CPU: 5.63 sec
HDFS Read: 361084006 HDFS Write: 357 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 630 msec
OK
500000 wyp.
Time taken: 13.042 seconds, Fetched: 1 row(s)
复制代码
时间用了13.042s这和没有创建索引的效果差不多。

在Hive创建索引还存在bug：如果表格的模式信息来自SerDe，Hive将不能创建索引：
hive> CREATE INDEX employees_index
    > ON TABLE employees (country)
    > AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
    > WITH DEFERRED REBUILD
    > IDXPROPERTIES ('creator' = 'me','created_at' = 'some_time')
    > IN TABLE employees_index_table
    > COMMENT 'Employees indexed by country and name.';
FAILED: Error in metadata: java.lang.RuntimeException:             \
Check the index columns, they should appear in the table being indexed.
FAILED: Execution Error, return code 1 from                       \
org.apache.hadoop.hive.ql.exec.DDLTask
复制代码

这个bug发生在Hive0.10.0、0.10.1、0.11.0，在Hive0.12.0已经修复了，详情请参见：https://issues.apache.org/jira/browse/HIVE-4251
