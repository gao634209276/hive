fffdMapReduce作业可以细分为map task和reduce task，而MRAppMaster又将map task和reduce task分为四种状态：

　　1、pending：刚启动但尚未向resourcemanager发送资源请求；

　　2、scheduled：已经向resourceManager发送资源请求，但尚未分配到资源；

　　3、assigned：已经分配到了资源且正在运行；

　　4、completed：已经运行完成。

　　map task的生命周期为：scheduled -> assigned -> completed
　　reduce task 生命周期：pending -> scheduled -> assigned -> completed。

　　由于reduce task的执行需要依赖于map task的输出结果，因此，为避免reduce task过早启动造成资源利用率底下，MRAppMaster让刚启动的reduce处于pending状态，以便能够根据map task的运行情况决定是否对其进行调度。

　　那么如何确定reduce task启动时机呢？因为YARN没有Hadoop 1.x里面的map slot和reduce slot概念，且ResourceManager也不知道map task和reduce task之间的依赖关系，因此MRAppMaster自己需要设计资源申请策略以防止因reduce task过早启动照成资源利用率低下和map task因分配不到资源而饿死。MRAppMaster在MRv1原有策略（map task完成数目达到一定比例后才允许启动reduce task）基础上添加了更为严格的资源控制策略和抢占策略，这里主要涉及到以下三个参数：

　　mapreduce.job.reduce.slowstart.completedmaps：其英文含义是：Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job。当map task完成的比例达到该值后才会为reduce task申请资源，默认是0.05。

　　yarn.app.mapreduce.am.job.reduce.rampup.limit：在map task完成之前，最多启动reduce task比例，默认是0.5

　　yarn.app.mapreduce.am.job.reduce.preemption.limit：当map task需要资源但暂时无法获取资源（比如reduce task运行过程中，部分map task因结果丢失需重算）时，为了保证至少一个map task可以得到资源，最多可以抢占reduce task比例，默认是0.5

　　如果上面三个参数设置的不合理可能会出现提交的job出现大量的reduce被kill掉，这个问题其实是reduce 任务启动时机的问题，由于yarn中没有map slot和reduce slot的概念，且ResourceManager也不知道map task和reduce task之间的依赖关系，因此MRAppMaster自己需要设计资源申请策略以防止因reduce task过早启动照成资源利用率低下和map task因分配不到资源而饿死，然后通过抢占机制，大量reduce任务被kill掉。可以合理调节上面三个配置参数来消除这种情况。

http://www.cnblogs.com/itboys/p/5769057.html

-------------------------------------------------------------
http://blog.javachen.com/2014/06/24/tuning-in-mapreduce/

本文主要记录Hadoop 2.x版本中MapReduce参数调优，不涉及Yarn的调优。

Hadoop的默认配置文件（以cdh5.0.1为例）：

    core-default.xml
    hdfs-default.xml
    mapred-default.xml

    说明：

    在hadoop2中有些参数名称过时了，例如原来的mapred.reduce.tasks改名为mapreduce.job.reduces了，当然，这两个参数你都可以使用，只是第一个参数过时了。

1. 操作系统调优

    增大打开文件数据和网络连接上限，调整内核参数net.core.somaxconn，提高读写速度和网络带宽使用率
    适当调整epoll的文件描述符上限，提高Hadoop RPC并发
    关闭swap。如果进程内存不足，系统会将内存中的部分数据暂时写入磁盘，当需要时再将磁盘上的数据动态换置到内存中，这样会降低进程执行效率
    增加预读缓存区大小。预读可以减少磁盘寻道次数和I/O等待时间
    设置openfile

2. Hdfs参数调优
2.1 core-default.xml：

hadoop.tmp.dir：

    默认值： /tmp
    说明： 尽量手动配置这个选项，否则的话都默认存在了里系统的默认临时文件/tmp里。并且手动配置的时候，如果服务器是多磁盘的，每个磁盘都设置一个临时文件目录，这样便于mapreduce或者hdfs等使用的时候提高磁盘IO效率。

fs.trash.interval：

    默认值： 0
    说明： 这个是开启hdfs文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。

io.file.buffer.size：

    默认值：4096
    说明：SequenceFiles在读写中可以使用的缓存大小，可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。

2.2 hdfs-default.xml：

dfs.blocksize：

    默认值：134217728
    说明： 这个就是hdfs里一个文件块的大小了，CDH5中默认128M。太大的话会有较少map同时计算，太小的话也浪费可用map个数资源，而且文件太小namenode就浪费内存多。根据需要进行设置。

dfs.namenode.handler.count：

    默认值：10
    说明：设定 namenode server threads 的数量，这些 threads 會用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加

3. MapReduce参数调优

包括以下节点：

    合理设置槽位数目
    调整心跳配置
    磁盘块配置
    设置RPC和线程数目
    启用批量任务调度

3.1 mapred-default.xml：

mapred.reduce.tasks（mapreduce.job.reduces）：

    默认值：1
    说明：默认启动的reduce数。通过该参数可以手动修改reduce的个数。

mapreduce.task.io.sort.factor：

    默认值：10
    说明：Reduce Task中合并小文件时，一次合并的文件数据，每次合并的时候选择最小的前10进行合并。

mapreduce.task.io.sort.mb：

    默认值：100
    说明： Map Task缓冲区所占内存大小。

mapred.child.java.opts：

    默认值：-Xmx200m
    说明：jvm启动的子线程可以使用的最大内存。建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc

mapreduce.jobtracker.handler.count：

    默认值：10
    说明：JobTracker可以启动的线程数，一般为tasktracker节点的4%。

mapreduce.reduce.shuffle.parallelcopies：

    默认值：5
    说明：reuduce shuffle阶段并行传输数据的数量。这里改为10。集群大可以增大。

mapreduce.tasktracker.http.threads：

    默认值：40
    说明：map和reduce是通过http进行数据传输的，这个是设置传输的并行线程数。

mapreduce.map.output.compress：

    默认值：false
    说明： map输出是否进行压缩，如果压缩就会多耗cpu，但是减少传输时间，如果不压缩，就需要较多的传输带宽。配合 mapreduce.map.output.compress.codec使用，默认是 org.apache.hadoop.io.compress.DefaultCodec，可以根据需要设定数据压缩方式。

mapreduce.reduce.shuffle.merge.percent：

    默认值： 0.66
    说明：reduce归并接收map的输出数据可占用的内存配置百分比。类似mapreduce.reduce.shuffle.input.buffer.percen属性。

mapreduce.reduce.shuffle.memory.limit.percent：

    默认值： 0.25
    说明：一个单一的shuffle的最大内存使用限制。

mapreduce.jobtracker.handler.count：

    默认值： 10
    说明：可并发处理来自tasktracker的RPC请求数，默认值10。

mapred.job.reuse.jvm.num.tasks（mapreduce.job.jvm.numtasks）：

    默认值： 1
    说明：一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。

mapreduce.tasktracker.tasks.reduce.maximum：

    默认值： 2
    说明：一个tasktracker并发执行的reduce数，建议为cpu核数

4. 系统优化
4.1 避免排序

对于一些不需要排序的应用，比如hash join或者limit n，可以将排序变为可选环节，这样可以带来一些好处：

    在Map Collect阶段，不再需要同时比较partition和key，只需要比较partition，并可以使用更快的计数排序（O(n)）代替快速排序（O(NlgN)）
    在Map Combine阶段，不再需要进行归并排序，只需要按照字节合并数据块即可。
    去掉排序之后，Shuffle和Reduce可同时进行，这样就消除了Reduce Task的屏障（所有数据拷贝完成之后才能执行reduce()函数）。

4.2 Shuffle阶段内部优化

    Map端--用Netty代替Jetty
    Reduce端--批拷贝
    将Shuffle阶段从Reduce Task中独立出来

5. 总结

在运行mapreduce任务中，经常调整的参数有：

    mapred.reduce.tasks：手动设置reduce个数
    mapreduce.map.output.compress：map输出结果是否压缩
        mapreduce.map.output.compress.codec
    mapreduce.output.fileoutputformat.compress：job输出结果是否压缩
        mapreduce.output.fileoutputformat.compress.type
        mapreduce.output.fileoutputformat.compress.codec


