hive表的存储格式
	TEXTFILE
	SEQUENCEFILE（三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩）
	RCFILE
	ORC
	自定义格式
Hadoop默认支持Gzip和BZip2的解压缩方式，可直接读取（hadoop fs -text命令），
但hive只能用TEXTFILE格式的表加载，然后再insert overwrite 到其他格式的表（比如SEQUENCEFILE表），
如果hive其他格式的表想要直接加载压缩格式数据，需要重写INPUTFORMAT和OUTPUTFORMAT文件类

压缩格式文件的切分(不支持则hadoop不能并行的进行map操作)
    BZip2和LZO（提供block级的压缩）支持文件切分
    Gzip和Snappy则不支持。

hadoop中支持的压缩格式
	DEFLATE org.apache.hadoop.io.compress.DefaultCodec
	gzip org.apache.hadoop.io.compress.GzipCodec
	bzip org.apache.hadoop.io.compress.BZip2Codec
	Snappy org.apache.hadoop.io.compress.SnappyCodec
	LZO org.apache.hadoop.io.compress.LzopCodec或者com.hadoop.compression.lzo.LzopCodec;
	org.apache.hadoop.io.compress.LzoCodec或者com.hadoop.compression.lzo.LzoCodec;
	(1)org.apache.hadoop.io.compress.LzoCodec和com.hadoop.compression.lzo.LzoCodec功能一样，
	都是源码包中带的，返回都是lzo_deflate文件
	(2)有两种压缩编码可用，即LzoCodec和LzopCodec，区别是：
		1)LzoCodec比LzopCodec更快， LzopCodec为了兼容LZOP程序添加了如 bytes signature, header等信息
		2)LzoCodec作为Reduce输出，结果文件扩展名为”.lzo_deflate”，无法被lzop读取；
		而使用LzopCodec作为Reduce输出，生成扩展名为”.lzo”的文件，可被lzop读取
		3)LzoCodec结果（.lzo_deflate文件)不能由lzo index job的"DistributedLzoIndexer"创建index；
		且“.lzo_deflate”文件不能作为MapReduce输入（不识别，除非自编inputformat）。而所有这些“.LZO”文件都支持
	综上所述，应该map输出的中间结果使用LzoCodec，reduce输出用LzopCodec
	重要的辅助工作，添加索引
		添加index是为让.lzo文件子在hdfs上按照block大小来切分块（速度加快，但多消耗cpu时间。map数大量增加）
		如果不建立lzo索引则不会按照block来切分块
	为每个lzo块添加index的命令：
	hadoop jar $HADOOP_HOME/lib/hadoop-lzo-0.4.15.jar com.hadoop.compression.lzo.DistributedLzoIndexer path/xxx.lzo
	注意（只设置mapred.output.compress=true默认的reduce输出格式为.lzo_deflate）


hive压缩的编解码器（压缩格式）
	set io.compression.codecs 可以查看目前hive已加载的所以编解码器
	io.compression.codecs 是hadoop的MR读写支持的所有格式支持，如果设置，就必须设置所有支持格式。
	默认支持，没有必要的话，最好别加。设置多个语法用逗号分割
	set mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec和
	set mapred.output.compression.codec=org.apache.hadoop.io.compress.LzopCodec
	两者一样，是LzopCodec的两个不同开源包。用哪个都行。

hive压缩设置
	1）中间结果压缩
		中间结果是map产生的。hive-site.xml格式设置语句
		set hive.exec.compress.intermediate=true;
		set hive.intermediate.compression.codec=org.apache.Hadoop.io.compress.LzoCodec;
		map结果压缩最好使用snappy的，因为压缩的前提是map输出非常大，影响io，如果中间结果数据集比较小反而会拖慢速度
		另外，中间结果的压缩格式设置还可以直接设置map输出结果压缩实现，如mapred-site.xml:
		set mapred.compress.map.output=ture
		set mapred.map.output.compression.codec=org.apache.Hadoop.io.compress.SnappyCodec
		来代替set hive.intermediate.compression.codec这个语句实现
	2）最终输出结果压缩
		hive-site.xml
			set hive.exec.compress.output=true
			set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec
		或hdfs-site.xml:
			set mapred.output.compress=true //不是hive执行sql的压缩,是mr的压缩
			set mapred.output.compression.codec=org.apache.hadoop.io.compress.LzopCodec
        两种方式功能一样，之所以两个方式，是因为作用不同的参数文件

example:
	CREATE TABLE ... STORED AS ORC
	tblproperties ("orc.compress"="SNAPPY");
	ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC
	create table lzo(name string)  STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
	OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';


	SET hive.default.fileformat=Orc
	mapred.map.output.compression.codec
	alter table t_user_3mon_tmp SET fileformat ORC;

	create table test_orc(name string)
	stored as orcfile tblproperties ("orc.compress"="SNAPPY");


create table test_tmp(name string);
load data local inpath '/app/sinova/file' overwrite into table test_tmp;

set mapred.output.compress=true;
set hive.exec.compress.output=true;
create table test_text(name string);
insert overwrite table test_text select * from test_tmp;
select * from test_text;
dfs -ls hive/warehouse/test_text/*;
dfs -text hive/warehouse/test_text/*;
drop table if exists test_text;

set hive.exec.compress.output=true;
set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
create table test_gz(name string);
insert overwrite table test_gz select * from test_tmp;
select * from test_gz;
dfs -ls hive/warehouse/test_gz/*;
dfs -text hive/warehouse/test_gz/*;
drop table if exists test_gz;

set hive.exec.compress.output=true;
set mapred.output.compress=true;
--set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
create table test_snappy(name string);
insert overwrite table test_snappy select * from test_tmp;
select * from test_snappy;
dfs -ls hive/warehouse/test_snappy/*;
dfs -text hive/warehouse/test_snappy/*;
drop table if exists test_snappy;

set hive.exec.compress.output=true;
set mapred.output.compress=true;
create table if not exists test_orc( all string )
STORED AS ORCFILE;
insert overwrite table test_orc select * from test_tmp;
select * from test_snappy;
dfs -ls hive/warehouse/test_orc/*;
dfs -text hive/warehouse/test_orc/*;
drop table if exists test_orc;

create table test_orc( all string )
ROW FORMAT DELIMITED  FIELDS TERMINATED BY ','
STORED AS ORCFILE TBLPROPERTIES ("orc.compress"="SNAPPY");
stored as ORC;
Sequence File是可分割的文件格式，支持Hadoop的block级压缩
SORTED AS SEQUENCEFILE;
Sequence Files有三个不同的压缩选项：NOE,RECORD和BLOCK。
RECORD是默认选项，通常BLOCK会带来较RECORD更好的压缩性能。
同很多其他压缩选项一样，这个压缩类型参数不是Hive特有的，需要在Hadoop中或在每一个Hive查询脚本中设置。
set mapred.output.compression.type=BLOCK;
SequenceFileOutputFormat;


lzo
create table test_lzo(name string)
STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';
insert overwrite table test_lzo select * from test_tmp;

create table test_seq(name string)
stored as sequencefile;
insert overwrite table test_seq select * from test_tmp;
